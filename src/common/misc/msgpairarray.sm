/* 
 * (C) 2003 Clemson University and The University of Chicago 
 *
 * See COPYING in top-level directory.
 */

/* pvfs2_msgpairarray_sm
 *
 * The purpose of this state machine is to prepare, send, and
 * receive a collection of request/response pairs (msgpairs).
 */

#include "pvfs2-internal.h"

#include <string.h>
#include <assert.h>

#include "msgpairarray.h"
#include "pvfs2-debug.h"
#include "pint-cached-config.h"
#include "job.h"
#include "gossip.h"
#include "PINT-reqproto-encode.h"
#include "pvfs2-util.h"
#include "pint-util.h"
#include "server-config-mgr.h"
#include "state-machine.h"

#define MPA_SUM_CT (mop->params.send_ct + mop->params.recv_ct + \
                    mop->params.flow_ct + mop->params.ack_ct)

#ifdef WIN32
#define gossip_err_unless_quiet(format, ...) \
do {\
    if(mop->params.quiet_flag)\
    {\
        gossip_debug(GOSSIP_MSGPAIR_DEBUG, format, __VA_ARGS__); \
    }\
    else \
    {\
        gossip_err(format, __VA_ARGS__); \
    }\
} while(0)

#else
#define gossip_err_unless_quiet(format, f...) \
do {\
    if(mop->params.quiet_flag)\
        gossip_debug(GOSSIP_MSGPAIR_DEBUG, format, ##f); \
    else \
        gossip_err(format, ##f); \
} while(0)
#endif

static inline int io_process_message_recv(PINT_sm_msgarray_op *mop,
                                          job_status_s *js_p);

static inline int io_decode_message_recv(PINT_sm_msgpair_state *msg_p,
                                         struct PINT_decoded_msg *decoded_msg,
                                         struct PVFS_server_resp **resp);

static inline int io_post_flow(PINT_smcb *smcb, PINT_sm_msgpair_state *msg_p);

enum
{
    REQ_COMPLETE = 189,
    MSGPAIRS_COMPLETE = 190,
    MSGPAIRS_RETRY_REQ = 191,
    MSGPAIRS_RETRY_FLOW = 192,
    MSGPAIRS_RETRY_NODELAY = 193
};

%%

nested machine pvfs2_msgpairarray_work_sm
{
    state post_recv_send
    {
        run msgpairarray_post_recv_send;
        success => complete_recv_send;
        default => msg_failure;
    }

    state complete_recv_send
    {
        run msgpairarray_complete_recv_send;
        REQ_COMPLETE => post_ack_flow;
        MSGPAIRS_COMPLETE => complete_msg;
        MSGPAIRS_RETRY_REQ => retry_recv_send;
        default => complete_recv_send;
    }

    state retry_recv_send
    {
        run msgpairarray_retry_recv_send;
        default => post_recv_send;
    }

    state post_ack_flow
    {
        run msgpairarray_post_ack_flow;
        success => complete_ack_flow;
        default => msg_failure;
    }

    state complete_ack_flow
    {
        run msgpairarray_complete_ack_flow;
        MSGPAIRS_COMPLETE => complete_msg;
        MSGPAIRS_RETRY_FLOW => retry_ack_flow;
        MSGPAIRS_RETRY_REQ => retry_recv_send;
        default => complete_ack_flow;
    }

    state retry_ack_flow
    {
        run msgpairarray_retry_ack_flow;
        default => post_ack_flow;
    }

    state complete_msg
    {
        run msgpairarray_complete_msg;
        MSGPAIRS_RETRY_REQ => retry_recv_send;
        default => terminate;
    }

    state msg_failure
    {
        run msgpairarray_failure;
        default => terminate;
    }
}

nested machine pvfs2_msgpairarray_sm
{
    state init
    {
        run msgpairarray_init;
        default => fork;
    }

    state fork
    {
        pjmp msgpairarray_fork
        {
            default => pvfs2_msgpairarray_work_sm;
        }
        default => fork_cleanup;
    }

    state fork_cleanup
    {
        run msgpairarray_fork_cleanup;
        default => done;
    }

    state done
    {
        run msgpairarray_done;
        default => return;
    }
}

%%

static PINT_sm_action msgpairarray_init(struct PINT_smcb *smcb,
                                        job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int i = 0;
    PINT_sm_msgpair_state *msg_p = NULL;

    gossip_debug(GOSSIP_MIRROR_DEBUG,"Executing msgpairarray_init...\n");
    gossip_debug(GOSSIP_MIRROR_DEBUG,"\tbase frame:%d\tframe count:%d\n"
                                    ,smcb->base_frame,smcb->frame_count);

    gossip_debug(GOSSIP_MSGPAIR_DEBUG, "(%p) msgpairarray state: init "
                 "(%d msgpair(s))\n", smcb, mop->count);

    PVFS_perror_gossip_verbose();

    assert(mop->count > 0);

    js_p->error_code = 0;

    /* rather than one global message count, we count different types of
     * messages that are active - increment when they post and decrement
     * when they complete
     */
    mop->params.send_ct = 0;
    mop->params.recv_ct = 0;
    mop->params.flow_ct = 0;
    mop->params.ack_ct = 0;

    for(i = 0; i < mop->count; i++)
    {
        msg_p = &mop->msgarray[i];
        assert(msg_p);

        assert((msg_p->retry_flag == PVFS_MSGPAIR_RETRY) ||
               (msg_p->retry_flag == PVFS_MSGPAIR_NO_RETRY));

        msg_p->encoded_resp_p = NULL;
        msg_p->retry_count = 0;
        msg_p->complete = 0;
        msg_p->op_status = 0;
        msg_p->op_action = PVFS_MPA_OK;

    }
    return SM_ACTION_COMPLETE;
}

static PINT_sm_action msgpairarray_fork(struct PINT_smcb *smcb,
                                        job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int i = 0;
    for(i = 0; i < mop->count; i++)
    {
        PINT_sm_msgarray_op *submop;
        submop = (PINT_sm_msgarray_op *)malloc(sizeof(PINT_sm_msgarray_op));
        submop->params = mop->params;
        submop->count = mop->count;
        submop->index = i;
        submop->msgarray = mop->msgarray;
        PINT_sm_push_frame(smcb, i + 1, submop); /* task 0 is invalid */
    }
    return SM_ACTION_COMPLETE;
}

static PINT_sm_action msgpairarray_fork_cleanup(struct PINT_smcb *smcb,
                                                job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int i = 0;
    for(i = 0; i < mop->count; i++)
    {
        PINT_sm_msgarray_op *submop;
        int task_id;
        int error_code;
        submop = (PINT_sm_msgarray_op *)PINT_sm_pop_frame(smcb,
                                                          &task_id,
                                                          &error_code,
                                                          NULL);
        /* V3 replace with a gossip_debug */
        /* TODO: put in more detailed error checking */
        if (submop->index != task_id - 1)
        {
            gossip_err("index mismatch from pjmp in MPA\n");
        }
        if (error_code != 0)
        {
            if ( error_code < 0 )
            {
               char buf[1024];
               PVFS_strerror_r(error_code, buf, 1024);
               gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                            "error code (%d) returned from pjmp in MPA: %s\n",
                            error_code, buf);
            }
        }
        if ((submop->count != mop->count && js_p->error_code == 0))
        {
            gossip_err("count mismatch from pjmp in MPA\n");
        }

        /* What if there more multiple return codes? */
        /* send all return codes to caller */
        js_p->error_code = error_code;

        /* everything important in the submops are held elsewhere 
         * free the memory allocated berfore the PJMP 
         */
        free(submop);
    }
    return SM_ACTION_COMPLETE;
}

static PINT_sm_action msgpairarray_done(struct PINT_smcb *smcb,
                                        job_status_s *js_p)
{
    int task_id, error_code, remaining;
    /* can we pop this before we return? */
    PVFS_perror_gossip_verbose();
    PINT_sm_pop_frame(smcb, &task_id, &error_code, &remaining);
    return SM_ACTION_COMPLETE;
}

/* <================ NESTED STATE ACTION FUNCTIONS ===============> */

/* msgpairarray_post_recv_send()
 *
 * The following elements of the PINT_sm_msgpair_state
 * should be valid prior to this state (for each msgpair in array):
 * - req (unencoded request)
 * - srv_addr of each element in msg array
 *
 * This state performs the following operations for each msgpair,
 * one at a time:
 * (1) encodes request
 * (2) calculates maximum response size
 * (3) allocates BMI memory for response data (encoded)
 * (4) gets a session tag for the pair of messages
 * (5) posts the receive of the response
 * (6) posts the send of the request
 * (7) stores job ids for later matching
 *
 */
static PINT_sm_action msgpairarray_post_recv_send(struct PINT_smcb *smcb,
                                                  job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    PINT_sm_msgpair_state *msg_p = &mop->msgarray[mop->index];
    int ret = -PVFS_EINVAL, i = 0, tmp = 0;
    struct server_configuration_s *server_config = NULL;
    PVFS_msg_tag_t session_tag;
    struct filesystem_configuration_s *cur_fs = NULL;
    int must_loop_encodings = 0;
    int local_enc_and_alloc = 0;

    gossip_debug(GOSSIP_MSGPAIR_DEBUG, "%s: sm %p "
                 "%d total message(s) with %d incomplete\n", __func__, smcb,
                 mop->count * 2, MPA_SUM_CT);

    js_p->error_code = 0;

    assert(mop->count > 0);
    assert(msg_p);
    /* assert(mop->params.comp_ct >= 2); */

    i = mop->index; /* we removed a for loop but i is still convenient */

#if 0
    /*
     * here we skip over the msgs that have already completed in
     * the case of being in the retry code path when it's ok
     */
    if (msg_p->complete)
    {
        continue;
    }
#endif

    msg_p->op_status = 0;

    /* if this is a retry message will already be encoded and
     * we will skip this section 
     */
    if (msg_p->encoded_resp_p == NULL)
    {
        if (msg_p->fs_id != PVFS_FS_ID_NULL)
        {
            server_config = PINT_server_config_mgr_get_config(msg_p->fs_id);
            assert(server_config);

            cur_fs = PINT_config_find_fs_id(server_config, msg_p->fs_id);
            PINT_server_config_mgr_put_config(server_config);
            assert(cur_fs);
            msg_p->enc_type = cur_fs->encoding;
        } 
        if (!ENCODING_IS_VALID(msg_p->enc_type))
        {
            PRINT_ENCODING_ERROR("supported", msg_p->enc_type);
            must_loop_encodings = 1;
            msg_p->enc_type = (ENCODING_INVALID_MIN + 1);
        }
        else if (!ENCODING_IS_SUPPORTED(msg_p->enc_type))
        {
            PRINT_ENCODING_ERROR("supported", msg_p->enc_type);
            must_loop_encodings = 1;
            msg_p->enc_type = ENCODING_SUPPORTED_MIN;
        }

    try_next_encoding:
        assert(ENCODING_IS_VALID(msg_p->enc_type));

        ret = PINT_encode(&msg_p->req,
                          PINT_ENCODE_REQ,
                          &msg_p->encoded_req,
                          msg_p->svr_addr,
                          msg_p->enc_type);
        if (ret != 0)
        {
            if (must_loop_encodings)
            {
                gossip_debug(GOSSIP_MSGPAIR_DEBUG, "Looping through "
                             "encodings [%d/%d]\n", msg_p->enc_type,
                             ENCODING_INVALID_MAX);

                msg_p->enc_type++;
                if (ENCODING_IS_VALID(msg_p->enc_type))
                {
                    goto try_next_encoding;
                }
            }
            gossip_lerr("msgpairarray_post: PINT_encode failed\n");
            js_p->error_code = ret; /* take default */
            return SM_ACTION_COMPLETE;
        }

        /* calculate max response msg size and allocate space */
        msg_p->max_resp_sz = PINT_encode_calc_max_size(PINT_ENCODE_RESP,
                                                       msg_p->req.op,
                                                       msg_p->enc_type);


        msg_p->encoded_resp_p = BMI_memalloc(msg_p->svr_addr,
                                             msg_p->max_resp_sz,
                                             BMI_RECV);

        if (msg_p->encoded_resp_p == NULL)
        {
            js_p->error_code = -PVFS_ENOMEM; /* take default */
            return SM_ACTION_COMPLETE;
        }
        local_enc_and_alloc = 1;
    }

    session_tag = PINT_util_get_next_tag();

    /*store the session tag for this msgpair, so the msgpair completion */
    /*function can pass it to the caller of msgpairarray.               */
    msg_p->session_tag = session_tag;

    gossip_debug(GOSSIP_MSGPAIR_DEBUG, "%s: sm %p msgpair %d: "
                 "posting recv\n", __func__, smcb, i);

    /* post receive of response; job_id stored in recv_id */
    ret = job_bmi_recv(msg_p->svr_addr,
                       msg_p->encoded_resp_p,
                       msg_p->max_resp_sz,
                       session_tag,
                       BMI_PRE_ALLOC,
                       smcb,
                       PVFS_MPA_RECV,
                       &msg_p->recv_status,
                       &msg_p->recv_id,
                       mop->params.job_context,
                       mop->params.job_timeout,
                       msg_p->req.hints);
    if (ret == 0)
    {
        /* perform a quick test to see if the recv failed before posting
         * the send; if it reports an error quickly then we can save the
         * confusion of sending a request for which we can't recv a
         * response
         */
        ret = job_test(msg_p->recv_id,
                       &tmp,
                       NULL,
                       &msg_p->recv_status,
                       0,
                       mop->params.job_context);
    }

    /* there is an assumption that job_test won't ever return a 1 as
     * immediate completion doesn't make sense
     */
    if ((ret < 0) || (ret == 1))
    {
        /* it is impossible for this recv to complete at this point
         * without errors; we haven't sent the request yet!
         */
        assert(ret < 0 || msg_p->recv_status.error_code != 0);
        if (ret < 0)
        {
            PVFS_perror_gossip("Post of receive failed", ret);
        }
        else
        {
            PVFS_perror_gossip("Receive immediately failed",
                        msg_p->recv_status.error_code);
        }

        msg_p->recv_id = 0;
        msg_p->send_status.error_code = msg_p->recv_status.error_code;

        if (local_enc_and_alloc)
        {
            PINT_encode_release(&msg_p->encoded_req, PINT_ENCODE_REQ);
            memset(&msg_p->encoded_req, 0, sizeof(msg_p->encoded_req));
            BMI_memfree(msg_p->svr_addr,
                        msg_p->encoded_resp_p,
                        msg_p->max_resp_sz,
                        BMI_RECV);
            msg_p->encoded_resp_p = NULL;
            local_enc_and_alloc = 0;
        }

        js_p->error_code = ret; /* take default */
        return SM_ACTION_COMPLETE;
    }

    /* if we reach here, the recv has been posted without failure, but
     * has not completed yet
     */
    assert(ret == 0);
    mop->params.recv_ct++;

    gossip_debug(GOSSIP_MSGPAIR_DEBUG, "%s: sm %p msgpair %d: "
                 "posting send\n", __func__, smcb, i);

    /* post send of request; job_id stored in send_id */
    ret = job_bmi_send_list(msg_p->encoded_req.dest,
                            msg_p->encoded_req.buffer_list,
                            msg_p->encoded_req.size_list,
                            msg_p->encoded_req.list_count,
                            msg_p->encoded_req.total_size,
                            session_tag,
                            msg_p->encoded_req.buffer_type,
                            1,
                            smcb,
                            PVFS_MPA_SEND,
                            &msg_p->send_status,
                            &msg_p->send_id,
                            mop->params.job_context,
                            mop->params.job_timeout,
                            msg_p->req.hints);

    if ((ret < 0) || ((ret == 1) && (msg_p->send_status.error_code != 0)))
    {
        if (ret < 0)
        {
            PVFS_perror_gossip("Post of send failed", ret);
        }
        else
        {
            PVFS_perror_gossip("Send immediately failed",
                               msg_p->send_status.error_code);
        }

        gossip_err_unless_quiet("Send error: cancelling recv.\n");

        job_bmi_cancel(msg_p->recv_id, mop->params.job_context);
            
        /* we still have to wait for recv completion, so keep going */
        msg_p->op_status = msg_p->send_status.error_code;
        msg_p->send_id = 0;
    }
    else if (ret == 1)
    {
        /* immediate successful completion */
        msg_p->send_id = 0;
    }
    else
    {
        /* successful post, no immediate completion */
        mop->params.send_ct++;
    }
 
    /* we are still waiting on operations to complete, next state
     * transition will handle them
     */
    return SM_ACTION_DEFERRED;
}

/* This state action inserts a delay before we go back and issue retry
 * message pairs - this allows transient errors to clear.
 * For V3 need to modify to look at situation and decide if we will
 * retry to fail over - if fail over need to adjust a couple fields and
 * probably skip the delay.  This should be doable in this state action.
 */
static PINT_sm_action msgpairarray_retry_recv_send(struct PINT_smcb *smcb,
                                                   job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    job_id_t tmp_id;

    gossip_debug(GOSSIP_MSGPAIR_DEBUG, "%s: sm %p, wait %d ms\n",
                 __func__, smcb, mop->params.retry_delay);

    js_p->error_code = 0;  /* do not leak MSGPAIRS_RETRY through to wait */
    return job_req_sched_post_timer(mop->params.retry_delay,
                                    smcb,
                                    0,
                                    js_p,
                                    &tmp_id,
                                    mop->params.job_context);
}

/* This state action runs when a message post is complete - could be
 * either a send or a receive posted.  This will run once for BOTH
 * posts.  Only when both posts are complete do we proceed to 
 * msgpairarray_complete_msg which reviews the results and initiates
 * retries if needed or post ack and flow for an IO.
 */
static PINT_sm_action msgpairarray_complete_recv_send(struct PINT_smcb *smcb,
                                                      job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    PINT_sm_msgpair_state *msg_p = &mop->msgarray[mop->index];
    gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                 "%s: sm %p status_user_tag %d msgarray_count %d\n",
                 __func__, smcb, (int) js_p->status_user_tag, mop->count);

#if 0
    /* match operation with something in the msgpair array */
    /* the first N tags are sends, the second N are receives */
    assert(js_p->status_user_tag < mop->count * 2);
#endif

    switch (js_p->status_user_tag)
    {
    case PVFS_MPA_RECV :
        /* The RECV completed */
        mop->params.recv_ct--;
        msg_p->recv_id = 0; /* indicates recv is completed */

        /* save the status into the slot for the receive */
        msg_p->recv_status = *js_p;

        /* save error (if we don't already have one) in op_status */
        if(msg_p->op_status == 0)
        {
            msg_p->op_status = js_p->error_code;
        }

        if(msg_p->recv_status.error_code && msg_p->send_id != 0)
        {
            /* we got a receive error, but send is still pending.  Cancel
             * the send
             */
            job_bmi_cancel(msg_p->send_id, mop->params.job_context);
        }
        break;
    case PVFS_MPA_SEND :
        /* The SEND completed */
        mop->params.send_ct--;
        msg_p->send_id = 0; /* indicates send is completed */

        /* save the status into the slot for the send */
        msg_p->send_status = *js_p;

        /* save error (if we don't already have one) in op_status */
        if(msg_p->op_status == 0)
        {
            msg_p->op_status = js_p->error_code;
        }

        if(msg_p->send_status.error_code && msg_p->recv_id != 0)
        {
            /* we got a send error, but recv is still pending.  Cancel
             * the recv 
             */
            job_bmi_cancel(msg_p->recv_id, mop->params.job_context);
        }
        break;
    default :
        /* error - unexpected job completed */
        gossip_err("msgpairarray complete send/recv got bad user tag\n");
        break;
    }

    /* check if both operations have completed */
    if (msg_p->send_id || msg_p->recv_id)
    {
        /* still more jobs need to complete */
        gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                     "  msgpairarray send/recv: %d operations remain\n",
                     MPA_SUM_CT);
        js_p->error_code = 0;
        return SM_ACTION_DEFERRED;
    }

    /* all jobs completed */
    gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                 "  msgpairarray send/recv: all operations complete\n");

    if (msg_p->msgclass == PVFS_IO_IO)
    {
        js_p->error_code = REQ_COMPLETE;
    }
    else
    {
        js_p->error_code = MSGPAIRS_COMPLETE;
    }
    return SM_ACTION_COMPLETE;
}

/* We have sent our IO request and received a go ahead so we first post
 * an ack recv if this is a WRITE, and then post the flow.
 */
static PINT_sm_action msgpairarray_post_ack_flow (struct PINT_smcb *smcb,
                                                  job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    PINT_sm_msgpair_state *msg_p = &mop->msgarray[js_p->status_user_tag];
    int ret = -PVFS_EINVAL;

    /* First decode the server response */
    ret = io_process_message_recv(mop, js_p);
    if (ret < 0)
    {
        char buf[64] = {0};
        PVFS_strerror_r(ret, buf, 64);

        gossip_debug(GOSSIP_IO_DEBUG,
                     "%s: io_process_message_recv failed: "
                     "%s (%d remaining msgpairs)\n",
                     __func__, buf, MPA_SUM_CT);
                      
        /* if recv failed, probably have to do the send again too */
        msg_p->op_action = PVFS_MPA_FAIL;
        js_p->error_code = ret; /* take default */
        return SM_ACTION_COMPLETE;
         
    }
    /* Next post write ACK recv if this is a write */
    if(msg_p->msgdir == PVFS_IO_WRITE)
    {
        /* if this is a WRITE there is another ACK at the end so we need
         * to set up to receive it
         */
        gossip_debug(GOSSIP_IO_DEBUG, "  preposting write "
                     "ack for context %p.\n", msg_p);

        msg_p->max_resp_sz = PINT_encode_calc_max_size(
                                                 PINT_ENCODE_RESP,
                                                 PVFS_SERV_WRITE_COMPLETION,
                                                 mop->flow_params->encoding);
        msg_p->encoded_resp_p = BMI_memalloc(msg_p->svr_addr,
                                             msg_p->max_resp_sz,
                                             BMI_RECV);

        if (!msg_p->encoded_resp_p)
        {
            gossip_err("BMI_memalloc (for write ack) failed\n");
            msg_p->op_action = PVFS_MPA_FAIL;
            js_p->error_code = ret; /* take default */
            return SM_ACTION_COMPLETE;
        }

        /* we're pre-posting the final write ack here, even though it's
         * ahead of the flow phase; reads are at the flow phase.
         *
         * the timeout used here is a scaling one that needs to be long
         * enough for the entire flow to occur
         */
        /* pre-post this recv with an infinite timeout and adjust it
         * after the flow completes since we don't know how long a flow
         * can take at this point
         */
        ret = job_bmi_recv(msg_p->svr_addr,
                           msg_p->encoded_resp_p,
                           msg_p->max_resp_sz,
                           msg_p->session_tag,
                           BMI_PRE_ALLOC,
                           smcb,
                           PVFS_MPA_ACK,
                           &msg_p->ack_status,
                           &msg_p->ack_id,
                           mop->params.job_context,
                           JOB_TIMEOUT_INF,
                           mop->flow_params->hints);

        if(ret < 0)
        {
            /* attempt to post failed outright */
            PVFS_perror_gossip("Post of write-ack recv failed", ret);
            msg_p->op_action = PVFS_MPA_FAIL;
            js_p->error_code = ret; /* take default */
            return SM_ACTION_COMPLETE;
        }   
        if (ret == 1)
        {
            /* we expect this write to _not_ succeed immediately,
             * because we have not posted the flow yet.
             */
            PVFS_perror_gossip("Post of write-ack recv failed immediate", ret);
            msg_p->op_action = PVFS_MPA_FAIL;
            js_p->error_code = msg_p->ack_status.error_code; /* default */
            return SM_ACTION_COMPLETE;
        }
        /* successful post */
        mop->params.ack_ct++;
    }   
    else
    {
        /* this is a read make darn sure the ack_id is zero */
        msg_p->ack_id = 0;
    }
        
    /* Next post the FLOW */
    ret = io_post_flow(smcb, msg_p);
    if(ret < 0)
    {
        /* this only happens if catestrophic failure */
        char buf[64] = {0};
        PVFS_strerror_r(ret, buf, 64);
            
        gossip_debug(GOSSIP_IO_DEBUG,
                     "%s: io_post_flow failed: "
                     "%s (%d remaining msgpairs)\n",
                     __func__, 
                     buf, 
                     MPA_SUM_CT);

        /* try to cancel the ack if it exists and return error */
        if (msg_p->ack_id != 0)
        {
            ret = job_bmi_cancel(msg_p->ack_id, mop->params.job_context);
            if (ret == 0)
            {
                mop->params.ack_ct--;
            }
        }

        /* in any case were are SNAFU so we need to get out */
        PVFS_perror_gossip("Flow post failed", ret);
        msg_p->op_action = PVFS_MPA_FAIL;
        js_p->error_code = ret; /* should take default */
        return SM_ACTION_COMPLETE;
    }
    /* flow posted */
    mop->params.flow_ct++;

    /* both ack and flow posted successfully */
    js_p->error_code = 0; /* success */
    return SM_ACTION_DEFERRED;
}

/* This state action prepares a retry for the flow and write ack
 */
static PINT_sm_action msgpairarray_retry_ack_flow (struct PINT_smcb *smcb,
                                                   job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);

    job_id_t tmp_id;

    gossip_debug(GOSSIP_MSGPAIR_DEBUG, "%s: sm %p, wait %d ms\n",
                 __func__, smcb, mop->params.retry_delay);

    js_p->error_code = 0;  /* do not leak MSGPAIRS_RETRY through to wait */
    return job_req_sched_post_timer(mop->params.retry_delay,
                                    smcb,
                                    0,
                                    js_p,
                                    &tmp_id,
                                    mop->params.job_context);
}

/* Either the ack or flow has completed.  Both must complete before we
 * move on. A read has no ack so be aware of that.
 */
static PINT_sm_action msgpairarray_complete_ack_flow (struct PINT_smcb *smcb,
                                                      job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    PINT_sm_msgpair_state *msg_p = &mop->msgarray[mop->index];
    int ret = -PVFS_EINVAL;

    switch (js_p->status_user_tag)
    {
    case PVFS_MPA_ACK :
        /* The ACK completed */
        mop->params.ack_ct--;
        msg_p->ack_id = 0;

        /* save error (if we don't already have one) in op_status */
        if(msg_p->op_status == 0)
        {
            msg_p->op_status = msg_p->ack_status.error_code;
        }

        if(msg_p->ack_status.error_code < 0)
        {
            /* receive of ack failed, try again */
            msg_p->op_action = PVFS_MPA_RETRY;
        }
        if(msg_p->ack_status.error_code && msg_p->flow_id != 0)
        {
            /* Server ack'd an error, but flow is still pending.
             * Cancel the flow.
             */
            job_flow_cancel(msg_p->flow_id, mop->params.job_context);
        }
        break;
    case PVFS_MPA_FLOW :
        /* The FLOW completed */
        mop->params.flow_ct--;
        msg_p->flow_id = 0;

        /* save error (if we don't already have one) in op_status */
        if(msg_p->op_status == 0)
        {
            msg_p->op_status = msg_p->flow_status.error_code;
        }

        if(msg_p->ack_id != 0)
        {
            /* ack isn't finished so set timeout for ack and
             * deal with errors when its done - why not cancel if error?
             */
            ret = job_reset_timeout(msg_p->ack_id, mop->params.job_timeout);
        }
        if(msg_p->flow_status.error_code && msg_p->ack_id == 0)
        {
            /* we got a flow error, no ack pending. */
            if ((PVFS_ERROR_CLASS(-js_p->error_code) == PVFS_ERROR_BMI) ||
                (PVFS_ERROR_CLASS(-js_p->error_code) == PVFS_ERROR_FLOW) ||
                (js_p->error_code == -ECONNRESET) ||
                (js_p->error_code == -PVFS_EPROTO))
            {
                /* retry the flow */
                msg_p->op_action = PVFS_MPA_RETRY;
            }
            else
            {
                /* don't retry */
                msg_p->op_action = PVFS_MPA_FAIL;
            }
        }
    default :
        /* error - unexpected job completed */
        gossip_err("msgpairarray complete fork/ack got bad user tag\n");
        break;
    }

    /* check if both operations have completed */
    if (msg_p->flow_id || msg_p->ack_id)
    {
        /* still more jobs need to complete */
        gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                     "  msgpairarray flow/ack: %d operations remain\n",
                     MPA_SUM_CT);
        js_p->error_code = 0; /* take default */
        return SM_ACTION_DEFERRED;
    }

    /* done with both decode our next state */
    switch (msg_p->op_action)
    {
    case PVFS_MPA_RETRY :
        js_p->error_code = MSGPAIRS_RETRY_FLOW;
        break;
    case PVFS_MPA_OK :
    case PVFS_MPA_FAIL :
        js_p->error_code = MSGPAIRS_COMPLETE;
        break;
    default :
        js_p->error_code = MSGPAIRS_COMPLETE;
        break;
    }

    /* all jobs completed */
    gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                 "  msgpairarray flow/ack: all operations complete\n");

    return SM_ACTION_COMPLETE;
}

/* This is not a true completion function but the state action that runs
 * when all of the posted jobs have completed.  It reviews results and
 * initiates retries if needed.  We submit posts in groups, then wait
 * for them to finish - if all complete the first time we will only run
 * this function once, but if we retry we may run this multiple times.
 */
static PINT_sm_action msgpairarray_complete_msg(struct PINT_smcb *smcb,
                                                job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    PINT_sm_msgpair_state *msg_p = &mop->msgarray[mop->index];
    int ret = -PVFS_EINVAL;
    struct PINT_decoded_msg decoded_msg;
    const char* server_string = NULL;

    /* response structure (decoded) */
    struct PVFS_server_resp *resp_p = NULL;

    /* we should get here on a SM_ACTION_COMPLETE so there should not be
     * an error was was not handled by the previous state
     */
    js_p->error_code = 0;

    gossip_debug(GOSSIP_MSGPAIR_DEBUG, "(%p) msgpairarray state: "
                 "completion_fn\n", smcb);
    gossip_debug(GOSSIP_MIRROR_DEBUG,
                 "Executing msgpairarray_completion_fn..\n");

    assert(msg_p);

    if (msg_p->op_status != 0)
    {
        char s[1024];
        /* this just deals with printing error warnings */
        PVFS_strerror_r(msg_p->op_status, s, sizeof(s));
        server_string = BMI_addr_rev_lookup(msg_p->svr_addr);
        if(!server_string)
        {
            server_string = "[UNKNOWN]";
        }

        gossip_err("Warning: msgpair failed to %s, will retry: %s\n",
                   server_string, s);

        /* this identifies we might need a retry but does not check
         * the actual error codes to see if it is a retriable
         * failure.
         */
        msg_p->op_action = PVFS_MPA_RETRY;
    }


    /* this decodes what should be a successful response */
    /* this could either be the recv ack or the write ack */
    /* this is an IO read, do we do this here? was it not done before? */
    ret = PINT_serv_decode_resp(msg_p->fs_id,
                                msg_p->encoded_resp_p,
                                &decoded_msg,
                                &msg_p->svr_addr,
                                msg_p->recv_status.actual_size,
                                &resp_p);
    if (ret != 0)
    {
        /* Didn't decode right, keep decode error */
        PVFS_perror_gossip("msgpairarray decode error", ret);
        msg_p->op_status = ret;
    }
    else
    {
        /* if we've made it this far, the server response status is
         * meaningful, so we save it.  This should be a valid response
         * so we should not overwrite any error codes here.
         */
        js_p->error_code = msg_p->op_status = resp_p->status;

        /* compute sizes based on message class */
        switch (msg_p->msgclass)
        {
        case PVFS_IO_IO :
            if (msg_p->msgdir == PVFS_IO_WRITE)
            {
                /* bytes written comes from write ack */
                mop->total_size += resp_p->u.write_completion.total_completed;
            }
            else if (msg_p->msgdir == PVFS_IO_READ)
            {
                /* bytes read comes from the flow */
                /* this moved from IO SM */
                mop->total_size += msg_p->flow_desc.total_transferred;
            }
            else
            {
                gossip_err("IO neither read nor write");
            }
            break;
        case PVFS_IO_SMALL_IO :
            mop->total_size += resp_p->u.small_io.result_size;
            mop->flow_params->dfile_size_array[mop->index] =
                                    resp_p->u.small_io.bstream_size;
            break;
        case PVFS_IO_METADATA :
            /* nothing to do here */
        default :
            break;
        }
    }

    /* NOTE: we call the function associated with each message,
     *       not just the one from the first array element.  so
     *       there could in theory be different functions for each
     *       message (to handle different types of messages all in
     *       the same array).
     */
    if (msg_p->comp_fn != NULL)
    {
        PVFS_error err_stat;

        gossip_debug(GOSSIP_MIRROR_DEBUG,"\texecuting msg_p->comp_fn..\n");
        /* If we call the completion function, store the result on
         * a per message pair basis.  Also store some non-zero
         * (failure) value in js_p->error_code if we see one.
         */
        err_stat = msg_p->comp_fn(smcb, resp_p, mop->index);

        if (msg_p->op_status == 0)
        {
            js_p->error_code = msg_p->op_status = err_stat;
        }
        /* even if we see a failure, continue to process with the
         * completion function. -- RobR
         */
        msg_p->op_action = PVFS_MPA_OK;
    }
    else if( (msg_p->op_status != 0) && (resp_p == NULL) )
    {
        /* before we reference resp_p we need to catch the case where 
         * PINT_serv_decode_resp didn't give us a struct. We error out
         * the same without referencing resp_p */
        gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                     "%s: error code %d from PINT_serv_decode_resp, "
                     "from server %d\n", __func__, msg_p->op_status, 
                     mop->index);
        js_p->error_code = msg_p->op_status;
        /* V3 We may want to revisit retrying some of these errors
         * with a replicant rather than just bailing out 
         */
        msg_p->op_action = PVFS_MPA_RETRY;
    }
    else if (resp_p->status != 0)
    {
        /* no comp_fn specified and status non-zero */
        gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                     "notice: msgpairarray_complete: error %d "
                     "from server %d\n", resp_p->status, mop->index);

        /* save a non-zero status to return if we see one */
        js_p->error_code = resp_p->status;

        /* If we don't have a completion function, there is no point
         * in continuing to process after seeing a failure.
         */
        /* V3 We may want to revisit retrying some of these errors
         * with a replicant rather than just bailing out 
         */
        msg_p->op_action = PVFS_MPA_RETRY;
    }

    /* free all the resources that we used to send and receive. */
    ret = PINT_serv_free_msgpair_resources(&msg_p->encoded_req,
                                           msg_p->encoded_resp_p,
                                           &decoded_msg,
                                           &msg_p->svr_addr,
                                           msg_p->max_resp_sz);

    /* cleanup capability
     * Maybe should be part of serv_free_msgpair_resources - V3
     */
    PINT_cleanup_capability(&msg_p->req.capability);

    if (ret)
    {
        PVFS_perror_gossip("Failed to free msgpair resources", ret);
        js_p->error_code = ret;
        goto errorout;
    }

 /* Old code - but why don't we ever set complete to 1?
  * This may be something we need to do, or may be obsolete - V3
  */
    /*
     * mark that this msgpair has been completed and should not be
     * retried in the case of possible future retries
     */
    msg_p->complete = 1;

    if (msg_p->op_action == PVFS_MPA_RETRY)
    {
        /* free up and zero out msg fields if we retry we will reallocate */
        memset(&msg_p->encoded_req, 0, sizeof(msg_p->encoded_req));
        msg_p->encoded_resp_p = NULL;
        msg_p->max_resp_sz = 0;
        /* keep any error code, ir we rewrite later, so be it */
        /* js_p->error_code = 0; */

        /* Now we check the error codes, retry count, and if retries
         * were requested to see if we really want to retry
         */
        if (msg_p->retry_flag == PVFS_MSGPAIR_RETRY &&
            PVFS_ERROR_CLASS(-msg_p->op_status) == PVFS_ERROR_BMI &&
            msg_p->retry_count < mop->params.retry_limit)
        {
            msg_p->retry_count++;
            gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                         "*** %s: msgpair %d failed, retry %d\n",
                         __func__, mop->index, msg_p->retry_count);
            if(msg_p->op_status == -BMI_ECANCEL)
            {
                /* if the error code indicates cancel, then skip the
                 * delay.  We have probably already been waiting a while
                 */
                gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                       "*** %s: msgpair skipping retry delay.\n", __func__);
                js_p->error_code = MSGPAIRS_RETRY_NODELAY;
            }
            else
            {
                gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                       "*** %s: msgpair retrying after delay.\n", __func__);
                js_p->error_code = MSGPAIRS_RETRY_REQ;
            }
        }
        else
        {
            /* Not going to retry so log it */
            char s[1024];
            server_string = BMI_addr_rev_lookup(msg_p->svr_addr);
            if(!server_string)
            {
                server_string = "[UNKNOWN]";
            }
            PVFS_strerror_r(msg_p->op_status, s, sizeof(s));
            gossip_err_unless_quiet(
                    "*** %s: msgpair to server %s failed: %s\n",
                    __func__, server_string, s);
            if(msg_p->retry_flag != PVFS_MSGPAIR_RETRY)
            {
                gossip_err_unless_quiet("*** No retries requested.\n");
            }
            else if(PVFS_ERROR_CLASS(-msg_p->op_status) != PVFS_ERROR_BMI)
            {
                gossip_err_unless_quiet("*** Non-BMI failure.\n");
            }
            else
            {
                gossip_err_unless_quiet("*** Out of retries.\n");
            }
            if (js_p->error_code == 0)
            {
                js_p->error_code = msg_p->op_status;
            }
        }
    }

    if (js_p->error_code == MSGPAIRS_RETRY_REQ)
    {
        return SM_ACTION_COMPLETE; /* Go back and retry the message */
    }

errorout:
    /* Last thing before message is complete is check for join */
    if (mop->join)
    {
#ifdef PVFS_MPA_JOIN_LOCK
        gen_mutex_lock(mop->join.mutex);
#endif
        mop->join->count++;
#ifdef PVFS_MPA_JOIN_LOCK
        gen_mutex_unlock(mop->join.mutex);
#endif
    }
    return SM_ACTION_TERMINATE; /* Done with this message / child SM */
}

/*  This message failed in a bad way we could not retry or otherwise
 *  recover from
 */
static PINT_sm_action msgpairarray_failure (struct PINT_smcb *smcb,
                                            job_status_s *js_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    PINT_sm_msgpair_state *msg_p = &mop->msgarray[mop->index];

    gossip_err("Failure in MsgPairArray %d\n", msg_p->op_status);
    /* something failed - free up anything we can make sure return codes
     * are good and then terminate
     */
    return SM_ACTION_TERMINATE;
}

/* <==================== HELPER FUNCTIONS ====================> */

/**
 * process_context_recv handles the response from the server
 * to the I/O request.  This is called for each I/O context
 * i.e. each specific server response for each datafile. 
 */
static inline int io_process_message_recv(PINT_sm_msgarray_op *mop,
                                          job_status_s *js_p) 
                                          
{
    PINT_sm_msgpair_state *msg_p = &mop->msgarray[mop->index];
    int ret = -PVFS_EINVAL;
    unsigned long status_user_tag = 0;
    struct PINT_decoded_msg decoded_msg;
    struct PVFS_server_resp *resp = NULL;

    gossip_debug(GOSSIP_IO_DEBUG,
                 "- io_process_message_recv called\n");

    assert(js_p);

    status_user_tag = (unsigned long)js_p->status_user_tag;

    if (js_p->error_code)
    {
        {
            char buf[1024];
            PVFS_strerror_r(js_p->error_code, buf, sizeof(buf));
            buf[sizeof(buf)-1] = '\0';
            gossip_debug(GOSSIP_IO_DEBUG, "%s: entered with error: %s\n",
                         __func__, buf);
        }
        return js_p->error_code;
    }

    /* decode the response from the server */
    ret = io_decode_message_recv(msg_p, &decoded_msg, &resp);
    if (ret)
    {
        {
            char buf[1024];
            PVFS_strerror_r(js_p->error_code, buf, sizeof(buf));
            buf[sizeof(buf)-1] = '\0';
            gossip_debug(GOSSIP_IO_DEBUG, "%s: failed: %s\n", __func__, buf);
        }
        return ret;
    }

    /* save the datafile size */
    switch(msg_p->msgclass)
    {
    case PVFS_IO_IO :
        mop->flow_params->dfile_size_array[mop->index] =
                                     resp->u.io.bstream_size;
        break;
    case PVFS_IO_SMALL_IO :
        mop->flow_params->dfile_size_array[mop->index] =
                                     resp->u.small_io.bstream_size;
        break;
    case PVFS_IO_METADATA :
        /* no dfile involved in METADATA operation */
        break;
    default :
        gossip_err("bad response decode in io_process_message_receive");
        break;
    }

    /* now we can destroy I/O request response resources */
    ret = PINT_serv_free_msgpair_resources(&msg_p->encoded_req,
                                           msg_p->encoded_resp_p,
                                           &decoded_msg,
                                           &msg_p->svr_addr,
                                           msg_p->max_resp_sz);

    msg_p->encoded_resp_p = NULL;
    PINT_cleanup_capability(&msg_p->req.capability);

    if (ret)
    {
        PVFS_perror_gossip("PINT_serv_free_msgpair_resources "
                           "failed", ret);
        return ret;
    }

    return ret;
}

static inline int io_decode_message_recv(PINT_sm_msgpair_state *msg_p,
                                         struct PINT_decoded_msg *decoded_msg,
                                         struct PVFS_server_resp **resp)
{
    int ret = -PVFS_EINVAL;

    gossip_debug(GOSSIP_IO_DEBUG, "- io_decode_message_reccv called\n");

    assert(msg_p && decoded_msg && resp);

    ret = PINT_serv_decode_resp(msg_p->fs_id,
                                msg_p->encoded_resp_p,
                                decoded_msg,
                                &msg_p->svr_addr,
                                msg_p->recv_status.actual_size,
                                resp);

    if (ret)
    {
        PVFS_perror("PINT_server_decode_resp failed", ret);
        return ret;
    }

    assert((*resp)->status < 1);
    msg_p->op_status = (*resp)->status;

    if (msg_p->recv_status.error_code || msg_p->op_status)
    {
        gossip_debug(
                GOSSIP_IO_DEBUG, "  error %d with status %d related "
                "to response from context %p; not submitting flow.\n",
                msg_p->recv_status.error_code,
                msg_p->op_status, msg_p);

        if (msg_p->recv_status.error_code)
        {
            PVFS_perror_gossip(
                "io_decode_message_reccv (recv_status.error_code)",
                msg_p->recv_status.error_code);
            ret = msg_p->recv_status.error_code;
        }
        else if (msg_p->op_status)
        {
            PVFS_perror_gossip("io_decode_message_reccv (op_status)",
                               msg_p->op_status);
            gossip_err("server: %s\n", BMI_addr_rev_lookup(msg_p->svr_addr));
            ret = msg_p->op_status;
        }

        PINT_serv_free_msgpair_resources(&msg_p->encoded_req,
                                         msg_p->encoded_resp_p,
                                         decoded_msg,
                                         &msg_p->svr_addr,
                                         msg_p->max_resp_sz);
            
        msg_p->encoded_resp_p = NULL;
        PINT_cleanup_capability(&msg_p->req.capability);

        if (ret)
        {
            PVFS_perror_gossip("PINT_serv_free_msgpair_resources "
                               "failed", ret);
            return ret;
        }
    }
    return ret;
}

/* post flow sets up the flow and posts it.  This may be called
 * either immediately after the request is posted (in the case of writes
 * at present), or not until the ack from the request is received (as
 * in the case of reads).
 */
static inline int io_post_flow(PINT_smcb *smcb, PINT_sm_msgpair_state *msg_p)
{
    PINT_sm_msgarray_op *mop = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int ret = 0;
    PVFS_object_attr *attr = NULL;
    struct server_configuration_s *server_config = NULL;
    struct filesystem_configuration_s *fs_config;

    gossip_debug(GOSSIP_IO_DEBUG, "%s: entry\n", __func__);

    if (!msg_p)
    {
        return -PVFS_EINVAL;
    }

    /* We need the file's metadata info (distribution and datafile count) */
    attr = mop->flow_params->attr;
    assert(attr);

    /* Notify BMI about the memory buffer the user passed in.  For
     * transports that need registration, this allows them to work 
     * with one large region rather than lots of small stripe-size 
     * regions.  But only bother if the request is contiguous; too 
     * complex and likely no faster in the highly fragmented case.
     */
    if (mop->flow_params->mem_req->num_contig_chunks == 1)
    {
        struct bmi_optimistic_buffer_info binfo;

        binfo.buffer = mop->flow_params->buffer;
        binfo.len = PINT_REQUEST_TOTAL_BYTES(mop->flow_params->mem_req),
        binfo.rw = mop->flow_params->io_type;
        BMI_set_info(msg_p->svr_addr, BMI_OPTIMISTIC_BUFFER_REG, &binfo);
    }

    gossip_debug(GOSSIP_IO_DEBUG, "* mem req size is %lld, "
                 "file_req size is %lld (bytes)\n",
                 lld(PINT_REQUEST_TOTAL_BYTES(mop->flow_params->mem_req)),
                 lld(PINT_REQUEST_TOTAL_BYTES(mop->flow_params->file_req)));

    /* must reset the error_code and internal PINT_distribute fields
     * in case of a retry */
    PINT_flow_reset(&msg_p->flow_desc);

    msg_p->flow_desc.file_data.fsize =
                    mop->flow_params->dfile_size_array[mop->index];
    msg_p->flow_desc.file_data.dist = attr->u.meta.dist;
    msg_p->flow_desc.file_data.server_nr = msg_p->server_nr;
    msg_p->flow_desc.file_data.server_ct = attr->u.meta.dfile_count;

    msg_p->flow_desc.file_req = mop->flow_params->file_req;
    msg_p->flow_desc.file_req_offset = mop->flow_params->file_req_offset;

    msg_p->flow_desc.mem_req = mop->flow_params->mem_req;

    msg_p->flow_desc.tag = msg_p->session_tag;
    msg_p->flow_desc.type = mop->flow_params->flowproto_type;
    msg_p->flow_desc.user_ptr = NULL;

    gossip_debug(GOSSIP_IO_DEBUG, "  bstream_size = %lld, datafile "
                 "nr=%d, ct=%d, file_req_off = %lld\n",
                 lld(msg_p->flow_desc.file_data.fsize),
                 msg_p->flow_desc.file_data.server_nr,
                 msg_p->flow_desc.file_data.server_ct,
                 lld(msg_p->flow_desc.file_req_offset));

    if (msg_p->msgdir == PVFS_IO_READ)
    {
        msg_p->flow_desc.file_data.extend_flag = 0;
        msg_p->flow_desc.src.endpoint_id = BMI_ENDPOINT;
        msg_p->flow_desc.src.u.bmi.address = msg_p->svr_addr;
        msg_p->flow_desc.dest.endpoint_id = MEM_ENDPOINT;
        msg_p->flow_desc.dest.u.mem.buffer = mop->flow_params->buffer;
    }
    else
    {
        assert(msg_p->msgdir == PVFS_IO_WRITE);

        msg_p->flow_desc.file_data.extend_flag = 1;
        msg_p->flow_desc.src.endpoint_id = MEM_ENDPOINT;
        msg_p->flow_desc.src.u.mem.buffer = mop->flow_params->buffer;
        msg_p->flow_desc.dest.endpoint_id = BMI_ENDPOINT;
        msg_p->flow_desc.dest.u.bmi.address = msg_p->svr_addr;
    }

    server_config = PINT_server_config_mgr_get_config(msg_p->fs_id);

    fs_config = PINT_config_find_fs_id(server_config, msg_p->fs_id);
    if(fs_config)
    {
        /* pick up any buffer settings overrides from fs conf */
        msg_p->flow_desc.buffer_size = fs_config->fp_buffer_size;
        msg_p->flow_desc.buffers_per_flow = fs_config->fp_buffers_per_flow;
    }

    ret = job_flow(&msg_p->flow_desc,
                   smcb,
                   PVFS_MPA_FLOW,
                   &msg_p->flow_status,
                   &msg_p->flow_id,
                   mop->params.job_context,
                   server_config->client_job_flow_timeout,
                   mop->flow_params->hints);

    PINT_server_config_mgr_put_config(server_config);

    /* if the flow fails immediately, then we have to do some special
     * handling.  This function is not equiped to handle the failure
     * directly, so we instead post a null job that will propigate the
     * error
     * to the normal state where we interpret flow errors
     */
    if((ret < 0) || (ret == 1 && msg_p->flow_status.error_code != 0))
    {
        /* make sure the error code is stored in the flow descriptor */
        if(ret == 1)
        {
            msg_p->flow_desc.error_code = msg_p->flow_status.error_code;
        }
        else
        {
            msg_p->flow_desc.error_code = ret;
        }

        gossip_debug(GOSSIP_IO_DEBUG, "  immediate flow failure for "
                     "context %p, error code: %d\n", msg_p,
                     msg_p->flow_desc.error_code);
        gossip_debug(GOSSIP_IO_DEBUG, "  posting job_null() to propigate.\n");

        /* post a fake job to propigate the flow failure to a later state */
        ret = job_null(msg_p->flow_desc.error_code,
                       smcb,
                       PVFS_MPA_FLOW,
                       &msg_p->flow_status,
                       &msg_p->flow_id,
                       mop->params.job_context);
        if(ret != 0)
        {
            return(ret);
        }
    }
    else
    {
        gossip_debug(GOSSIP_IO_DEBUG, "  posted flow for "
                     "context %p\n", msg_p);
    }

    mop->params.flow_ct++;
    return 0;
}

#if 0
/*
  returns 0 on send completion; IO_RECV_COMPLETED on recv completion,
  and -PVFS_error on failure
*/
static inline int io_complete_context_send_or_recv(PINT_smcb *smcb,
                                                   job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, index = 0;
    unsigned long status_user_tag = 0;
    /* GET RID OF CONTEXTS */
    PINT_client_io_ctx *cur_ctx = NULL;
    PINT_sm_msgpair_state *msg = NULL;
    /* BROKEN - frame is a mop */
    PINT_client_sm *sm_p = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);

    gossip_debug(GOSSIP_IO_DEBUG,
                 "- complete_context_send_or_recv called\n");

    assert(smcb && js_p);
    assert(smcb->op == PVFS_SYS_IO);

    status_user_tag = (unsigned long)js_p->status_user_tag;

    if (STATUS_USER_TAG_TYPE(status_user_tag, IO_SM_PHASE_REQ_MSGPAIR_RECV))
    {
        index = STATUS_USER_TAG_GET_INDEX(status_user_tag,
                                          IO_SM_PHASE_REQ_MSGPAIR_RECV);

        gossip_debug(GOSSIP_IO_DEBUG, "got a recv completion with "
                     "context index %d\n", index);

        cur_ctx = &sm_p->u.io.contexts[index];
        assert(cur_ctx);

        msg = &cur_ctx->msg;

        msg->recv_id = 0;
        msg->recv_status = *js_p;

        assert(msg->recv_status.error_code <= 0);
        assert(msg->recv_status.actual_size <= msg->max_resp_sz);

        cur_ctx->msg_recv_in_progress = 0;
        sm_p->u.io.msgpair_completion_count--;

        ret = IO_RECV_COMPLETED;
    }
    else if (STATUS_USER_TAG_TYPE(status_user_tag,
                                  IO_SM_PHASE_REQ_MSGPAIR_SEND))
    {
        index = STATUS_USER_TAG_GET_INDEX(status_user_tag,
                                          IO_SM_PHASE_REQ_MSGPAIR_RECV);

        gossip_debug(GOSSIP_IO_DEBUG, "got a send completion with "
                     "context index %d\n", index);

        cur_ctx = &sm_p->u.io.contexts[index];
        assert(cur_ctx);

        msg = &cur_ctx->msg;

        msg->send_id = 0;
        msg->send_status = *js_p;

        assert(msg->send_status.error_code <= 0);

        cur_ctx->msg_send_in_progress = 0;
        sm_p->u.io.msgpair_completion_count--;

        ret = 0;
    }
    return ret;
}

static inline int io_check_context_status(PINT_client_io_ctx *cur_ctx,
                                          int io_type,
                                          PVFS_size *total_size)
{
    int ret = 0;

    gossip_debug(GOSSIP_IO_DEBUG, "- io_check_context_status called\n");

    assert(cur_ctx && total_size);

    if (cur_ctx->msg.send_status.error_code)
    {
        gossip_debug(GOSSIP_IO_DEBUG,
                     "  error (%d) in msgpair send for context %p\n",
                     cur_ctx->msg.send_status.error_code, cur_ctx);
        ret = cur_ctx->msg.send_status.error_code;
    }
    else if (cur_ctx->msg.recv_status.error_code)
    {
        gossip_debug(GOSSIP_IO_DEBUG,
                     "  error (%d) in msgpair recv for context %p\n",
                     cur_ctx->msg.recv_status.error_code, cur_ctx);
        ret = cur_ctx->msg.recv_status.error_code;
    }
    else if (io_type == PVFS_IO_WRITE)
    {
        /* we check the write ack status before the flow status so that the
         * error code that the server reported in the ack takes precedence
         */
        if (cur_ctx->write_ack.recv_status.error_code)
        {
            gossip_debug(
                GOSSIP_IO_DEBUG,
                "  error (%d) in final ack for context %p\n",
                cur_ctx->write_ack.recv_status.error_code, cur_ctx);

            assert(cur_ctx->write_ack_has_been_posted);
            ret = cur_ctx->write_ack.recv_status.error_code;
        }
        else if (cur_ctx->write_ack_has_been_posted)
        {
            struct PINT_decoded_msg decoded_msg;
            struct PVFS_server_resp *resp = NULL;
            /*
              size for writes are reported in the final ack, but we
              have to decode it first
            */
            ret = PINT_serv_decode_resp(
                                     cur_ctx->msg.fs_id,
                                     cur_ctx->write_ack.encoded_resp_p,
                                     &decoded_msg,
                                     &cur_ctx->msg.svr_addr,
                                     cur_ctx->write_ack.recv_status.actual_size,
                                     &resp);
            if (ret == 0)
            {
                gossip_debug(
                    GOSSIP_IO_DEBUG,
                    "  %lld bytes written to context %p\n",
                    lld(resp->u.write_completion.total_completed),
                    cur_ctx);

                *total_size += resp->u.write_completion.total_completed;
                
                /* pass along the error code from the server as well */
                ret = resp->status;

                PINT_decode_release(&decoded_msg, PINT_DECODE_RESP);
            }
            else
            {
                PVFS_perror_gossip("PINT_serv_decode_resp failed", ret);
            }

            PINT_flow_reset(&cur_ctx->flow_desc);
            BMI_memfree(cur_ctx->msg.svr_addr,
                        cur_ctx->write_ack.encoded_resp_p,
                        cur_ctx->write_ack.max_resp_sz,
                        BMI_RECV);
        }
        else if (cur_ctx->flow_status.error_code)
        {
            gossip_debug(GOSSIP_IO_DEBUG,
                         "  error (%d) in flow for context %p\n",
                         cur_ctx->flow_status.error_code, cur_ctx);
            PINT_flow_reset(&cur_ctx->flow_desc);
            ret = cur_ctx->flow_status.error_code;
        }
    }
    else if (cur_ctx->flow_status.error_code)
    {
        gossip_debug(GOSSIP_IO_DEBUG,
                     "  error (%d) in flow for context %p\n",
                     cur_ctx->flow_status.error_code, cur_ctx);
        PINT_flow_reset(&cur_ctx->flow_desc);
        ret = cur_ctx->flow_status.error_code;
    }
    else if (io_type == PVFS_IO_READ)
    {
        gossip_debug(
            GOSSIP_IO_DEBUG, "  %lld bytes read from context %p\n",
            lld(cur_ctx->flow_desc.total_transferred), cur_ctx);

        /* size for reads are reported in the flow */
        *total_size += cur_ctx->flow_desc.total_transferred;

        /*
          we can't reset the flow here in case we have to do a zero
          fill adjustment that we haven't detected yet
        */
    }

    return ret;
}
#endif

/*********************************************************************
 * externally callable helper functions used in conjunction with 
 * the msgpairarray state machine defined above
 */

int PINT_msgpairarray_init(PINT_sm_msgarray_op *op, int count)
{
    op->msgarray = (PINT_sm_msgpair_state *)malloc(count *
                                            sizeof(PINT_sm_msgpair_state));
    if(!op->msgarray)
    {
        return -PVFS_ENOMEM;
    }
    memset(op->msgarray, 0, (count * sizeof(PINT_sm_msgpair_state)));
    op->count = count;

    return 0;
}

/* we pass in a pointer to the array so that we can set it to NULL */
void PINT_msgpairarray_destroy(PINT_sm_msgarray_op *op)
{
    if(op->msgarray && (&op->msgpair) != op->msgarray)
    {
        free(op->msgarray);
    }
    if (op->flow_params)
    {
        free(op->flow_params);
    }
    op->flow_params = NULL;
    op->msgarray = NULL;
    op->count = 0;
}

int PINT_msgarray_status(PINT_sm_msgarray_op *op)
{
    int i;
    for (i = 0; i < op->count; i++) 
    {
        if (op->msgarray[i].op_status != 0)
        {
            return op->msgarray[i].op_status;
        }
    }
    return 0;
}

int PINT_serv_decode_resp(PVFS_fs_id fs_id,
                          void *encoded_resp_p,
                          struct PINT_decoded_msg *decoded_msg_p,
                          PVFS_BMI_addr_t *svr_addr_p,
                          int actual_resp_sz,
                          struct PVFS_server_resp **resp_out_pp)
{
    int ret = -1;
    const char *server_string;


    ret = PINT_decode(encoded_resp_p,
                      PINT_DECODE_RESP,
                      decoded_msg_p, /* holds data on decoded msg */
                      *svr_addr_p,
                      actual_resp_sz);
    if (ret > -1)
    {
        *resp_out_pp = (struct PVFS_server_resp *)decoded_msg_p->buffer;
        if ((*resp_out_pp)->op == PVFS_SERV_PROTO_ERROR)
        {

            gossip_err("Error: server does not seem to understand "
                       "the protocol that this client is using.\n");
            gossip_err("   Please check server logs for more "
                       "information.\n");

            if (fs_id != PVFS_FS_ID_NULL)
            {
                server_string = BMI_addr_rev_lookup(*svr_addr_p);
                if(!server_string)
                {
                    server_string = "[UNKNOWN]";
                }
                gossip_err("   Server: %s.\n", server_string);
            }
            else
            {
                gossip_err("   Server: unknown; probably an error "
                           "contacting server listed in pvfs2tab "
                           "file.\n");
            }
            return(-EPROTONOSUPPORT);
        }
    }
    return ret;
}

int PINT_serv_free_msgpair_resources(struct PINT_encoded_msg *encoded_req_p,
                                     void *encoded_resp_p,
                                     struct PINT_decoded_msg *decoded_msg_p,
                                     PVFS_BMI_addr_t *svr_addr_p,
                                     int max_resp_sz)
{
    int ret = -PVFS_EINVAL;

    if (encoded_req_p &&
        encoded_resp_p &&
        decoded_msg_p &&
        svr_addr_p)
    {
        PINT_encode_release(encoded_req_p, PINT_ENCODE_REQ);
        memset(encoded_req_p, 0, sizeof(*encoded_req_p));

        /* this should release the msg struct but not the resp buffer
         * it points to
         */
        PINT_decode_release(decoded_msg_p, PINT_DECODE_RESP);
        memset(decoded_msg_p, 0, sizeof(*decoded_msg_p));

        BMI_memfree(*svr_addr_p, encoded_resp_p, max_resp_sz, BMI_RECV);
        encoded_resp_p = NULL;

        ret = 0;
    }
    return ret;
}

/* V3 this isn't used any more, but could be resurected if needed */
#if 0
/* PINT_serv_msgpair_array_resolve_addrs()
 *
 * fills in BMI address of server for each entry in the msgpair array,
 * based on the handle and fsid
 *
 * returns 0 on success, -PVFS_error on failure
 */
int PINT_serv_msgpairarray_resolve_addrs(PINT_sm_msgarray_op *mop)
{
    int i = 0;
    int ret = -PVFS_EINVAL;

    if ((mop->count > 0) && mop->msgarray)
    {
        for(i = 0; i < mop->count; i++)
        {
            PINT_sm_msgpair_state *msg_p = &mop->msgarray[i];
            assert(msg_p);

            ret = PINT_cached_config_map_to_server(&msg_p->svr_addr,
                                                   msg_p->handle,
                                                   msg_p->fs_id);

            if (ret != 0)
            {
                gossip_err("Failed to map server address to handle\n");
                break;
            }

            gossip_debug(GOSSIP_MSGPAIR_DEBUG,
                         " mapped handle %s to server %lld\n",
                         PVFS_OID_str(&msg_p->handle), lld(msg_p->svr_addr));
        }
    }
    return ret;
}
#endif

/*
 * Local variables:
 *  mode: c
 *  c-indent-level: 4
 *  c-basic-offset: 4
 * End:
 *
 * vim: ft=c ts=8 sts=4 sw=4 expandtab
 */

